<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>python_pachyderm.mixin.pps API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>python_pachyderm.mixin.pps</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import json
import base64
import warnings
from pathlib import Path

from python_pachyderm.proto.pps import pps_pb2 as pps_proto
from python_pachyderm.service import Service
from .util import commit_from


class PPSMixin:
    def inspect_job(self, job_id, block_state=None, output_commit=None, full=None):
        &#34;&#34;&#34;
        Inspects a job with a given ID. Returns a `JobInfo`.

        Params:

        * `job_id`: The ID of the job to inspect.
        * `block_state`: If true, block until the job completes.
        * `output_commit`: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        * `full`: If true, include worker status.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;InspectJob&#34;,
            job=pps_proto.Job(id=job_id),
            block_state=block_state,
            output_commit=commit_from(output_commit)
            if output_commit is not None
            else None,
            full=full,
        )

    def list_job(
        self,
        pipeline_name=None,
        input_commit=None,
        output_commit=None,
        history=None,
        full=None,
        jqFilter=None,
    ):
        &#34;&#34;&#34;
        Lists jobs. Yields `JobInfo` objects.

        Params:

        * `pipeline_name`: An optional string representing a pipeline name to
          filter on.
        * `input_commit`: An optional list of tuples, strings, or `Commit`
          objects representing input commits to filter on.
        * `output_commit`: An optional tuple, string, or `Commit` object
          representing an output commit to filter on.
        * `history`: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        * `full`: An optional bool indicating whether the result should
          include all pipeline details in each `JobInfo`, or limited information
          including name and status, but excluding information in the pipeline
          spec. Leaving this `None` (or `False`) can make the call significantly
          faster in clusters with a large number of pipelines and jobs. Note
          that if `input_commit` is set, this field is coerced to `True`.
        * `jqFilter`: An optional string containing a `jq` filter that can
          restrict the list of jobs returned, for convenience
        &#34;&#34;&#34;
        if isinstance(input_commit, list):
            input_commit = [commit_from(ic) for ic in input_commit]
        elif input_commit is not None:
            input_commit = [commit_from(input_commit)]

        return self._req(
            Service.PPS,
            &#34;ListJob&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name)
            if pipeline_name is not None
            else None,
            input_commit=input_commit,
            output_commit=commit_from(output_commit)
            if output_commit is not None
            else None,
            history=history,
            full=full,
            jqFilter=jqFilter,
        )

    def flush_job(self, commits, pipeline_names=None):
        &#34;&#34;&#34;
        Blocks until all of the jobs which have a set of commits as
        provenance have finished. Yields `JobInfo` objects.

        Params:

        * `commits`: A list of tuples, strings, or `Commit` objects
        representing the commits to flush.
        * `pipeline_names`: An optional list of strings specifying pipeline
        names. If specified, only jobs within these pipelines will be flushed.
        &#34;&#34;&#34;
        if pipeline_names is not None:
            to_pipelines = [pps_proto.Pipeline(name=name) for name in pipeline_names]
        else:
            to_pipelines = None

        return self._req(
            Service.PPS,
            &#34;FlushJob&#34;,
            commits=[commit_from(c) for c in commits],
            to_pipelines=to_pipelines,
        )

    def delete_job(self, job_id):
        &#34;&#34;&#34;
        Deletes a job by its ID.

        Params:

        * `job_id`: The ID of the job to delete.
        &#34;&#34;&#34;
        return self._req(Service.PPS, &#34;DeleteJob&#34;, job=pps_proto.Job(id=job_id))

    def stop_job(self, job_id, output_commit=None):
        &#34;&#34;&#34;
        Stops a job by its ID.

        Params:

        * `job_id`: The ID of the job to stop.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;StopJob&#34;,
            job=pps_proto.Job(id=job_id),
            output_commit=commit_from(output_commit),
        )

    def inspect_datum(self, job_id, datum_id):
        &#34;&#34;&#34;
        Inspects a datum. Returns a `DatumInfo` object.

        Params:

        * `job_id`: The ID of the job.
        * `datum_id`: The ID of the datum.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;InspectDatum&#34;,
            datum=pps_proto.Datum(id=datum_id, job=pps_proto.Job(id=job_id)),
        )

    def list_datum(self, job_id=None):
        &#34;&#34;&#34;
        Lists datums. Yields `DatumInfo` objects.

        Params:

        * `job_id`: An optional int specifying the ID of a job. Exactly one of
          `job_id` (real) or `input` (hypothetical) must be set.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;ListDatum&#34;,
            job=pps_proto.Job(id=job_id),
        )

    def restart_datum(self, job_id, data_filters=None):
        &#34;&#34;&#34;
        Restarts a datum.

        Params:

        * `job_id`: The ID of the job.
        * `data_filters`: An optional iterable of strings.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;RestartDatum&#34;,
            job=pps_proto.Job(id=job_id),
            data_filters=data_filters,
        )

    def create_pipeline(
        self,
        pipeline_name,
        transform,
        parallelism_spec=None,
        egress=None,
        reprocess_spec=None,
        update=None,
        output_branch=None,
        resource_requests=None,
        resource_limits=None,
        input=None,
        description=None,
        cache_size=None,
        enable_stats=None,
        reprocess=None,
        max_queue_size=None,
        service=None,
        chunk_spec=None,
        datum_timeout=None,
        job_timeout=None,
        salt=None,
        standby=None,
        datum_tries=None,
        scheduling_spec=None,
        pod_patch=None,
        spout=None,
        spec_commit=None,
        metadata=None,
        s3_out=None,
        sidecar_resource_limits=None,
    ):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `transform`: A `Transform` object.
        * `parallelism_spec`: An optional `ParallelismSpec` object.
        * `egress`: An optional `Egress` object.
        * `update`: An optional bool specifying whether this should behave as
        an upsert.
        * `output_branch`: An optional string representing the branch to output
        results on.
        * `resource_requests`: An optional `ResourceSpec` object.
        * `resource_limits`: An optional `ResourceSpec` object.
        * `input`: An optional `Input` object.
        * `description`: An optional string describing the pipeline.
        * `cache_size`: An optional string.
        * `enable_stats`: An optional bool.
        * `reprocess`: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * `max_queue_size`: An optional int.
        * `service`: An optional `Service` object.
        * `chunk_spec`: An optional `ChunkSpec` object.
        * `datum_timeout`: An optional `Duration` object.
        * `job_timeout`: An optional `Duration` object.
        * `salt`: An optional string.
        * `standby`: An optional bool.
        * `datum_tries`: An optional int.
        * `scheduling_spec`: An optional `SchedulingSpec` object.
        * `pod_patch`: An optional string.
        * `spout`: An optional `Spout` object.
        * `spec_commit`: An optional `Commit` object.
        * `metadata`: An optional `Metadata` object.
        * `s3_out`: An optional bool specifying whether the output repo should
        be exposed as an s3 gateway bucket.
        * `sidecar_resource_limits`: An optional `ResourceSpec` setting
        * `reprocess_spec`: An optional string specifying how to handle
        already-processed data
        resource limits for the pipeline sidecar.
        &#34;&#34;&#34;

        # Support for build step-enabled pipelines. This is a python port of
        # the equivalent functionality in pachyderm core&#39;s
        # &#39;src/server/pps/cmds/cmds.go&#39;, and any changes made here likely have
        # to be reflected there as well.
        if transform.build.image or transform.build.language or transform.build.path:
            if spout:
                raise Exception(&#34;build step-enabled pipelines do not work with spouts&#34;)
            if not input:
                raise Exception(&#34;no `input` specified&#34;)
            if (not transform.build.language) and (not transform.build.image):
                raise Exception(&#34;must specify either a build `language` or `image`&#34;)
            if transform.build.language and transform.build.image:
                raise Exception(&#34;cannot specify both a build `language` and `image`&#34;)
            if any(
                i.pfs is not None and i.pfs.name in (&#34;build&#34;, &#34;source&#34;)
                for i in pipeline_inputs(input)
            ):
                raise Exception(
                    &#34;build step-enabled pipelines cannot have inputs with the name &#34;
                    + &#34;&#39;build&#39; or &#39;source&#39;, as they are reserved for build assets&#34;
                )

            build_path = Path(transform.build.path or &#34;.&#34;)
            if not build_path.exists():
                raise Exception(&#34;build path {} does not exist&#34;.format(build_path))
            if (build_path / &#34;.pachignore&#34;).exists():
                warnings.warn(
                    &#34;detected a &#39;.pachignore&#39; file, but it&#39;s unsupported by python_pachyderm -- use `pachctl` instead&#34;,
                    RuntimeWarning,
                )

            build_pipeline_name = &#34;{}_build&#34;.format(pipeline_name)

            image = transform.build.image
            if not image:
                version = self.get_remote_version()
                version_str = &#34;{}.{}.{}{}&#34;.format(
                    version.major, version.minor, version.micro, version.additional
                )
                image = &#34;pachyderm/{}-build:{}&#34;.format(
                    transform.build.language, version_str
                )
            if not transform.image:
                transform.image = image

            def create_build_pipeline_input(name):
                return pps_proto.Input(
                    pfs=pps_proto.PFSInput(
                        name=name,
                        glob=&#34;/&#34;,
                        repo=build_pipeline_name,
                        branch=name,
                    )
                )

            self.create_repo(build_pipeline_name, update=True)

            self._req(
                Service.PPS,
                &#34;CreatePipeline&#34;,
                pipeline=pps_proto.Pipeline(name=build_pipeline_name),
                transform=pps_proto.Transform(image=image, cmd=[&#34;sh&#34;, &#34;./build.sh&#34;]),
                parallelism_spec=pps_proto.ParallelismSpec(constant=1),
                input=create_build_pipeline_input(&#34;source&#34;),
                output_branch=&#34;build&#34;,
                update=update,
            )

            with self.modify_file_client((build_pipeline_name, &#34;source&#34;)) as pfc:
                if update:
                    pfc.delete_file(&#34;/&#34;)
                for root, _, filenames in os.walk(str(build_path)):
                    for filename in filenames:
                        source_filepath = os.path.join(root, filename)
                        dest_filepath = os.path.join(
                            &#34;/&#34;, os.path.relpath(source_filepath, start=str(build_path))
                        )
                        pfc.put_file_from_filepath(dest_filepath, source_filepath)

            input = pps_proto.Input(
                cross=[
                    create_build_pipeline_input(&#34;source&#34;),
                    create_build_pipeline_input(&#34;build&#34;),
                    input,
                ]
            )

            if not transform.cmd:
                transform.cmd[:] = [&#34;sh&#34;, &#34;/pfs/build/run.sh&#34;]

        return self._req(
            Service.PPS,
            &#34;CreatePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            transform=transform,
            parallelism_spec=parallelism_spec,
            egress=egress,
            update=update,
            output_branch=output_branch,
            resource_requests=resource_requests,
            resource_limits=resource_limits,
            input=input,
            description=description,
            cache_size=cache_size,
            enable_stats=enable_stats,
            reprocess=reprocess,
            max_queue_size=max_queue_size,
            metadata=metadata,
            service=service,
            chunk_spec=chunk_spec,
            datum_timeout=datum_timeout,
            job_timeout=job_timeout,
            salt=salt,
            standby=standby,
            datum_tries=datum_tries,
            scheduling_spec=scheduling_spec,
            pod_patch=pod_patch,
            spout=spout,
            spec_commit=spec_commit,
            sidecar_resource_limits=sidecar_resource_limits,
            reprocess_spec=reprocess_spec,
        )

    def create_pipeline_from_request(self, req):
        &#34;&#34;&#34;
        Creates a pipeline from a `CreatePipelineRequest` object. Usually this
        would be used in conjunction with `util.parse_json_pipeline_spec` or
        `util.parse_dict_pipeline_spec`. If you&#39;re in pure python and not
        working with a pipeline spec file, the sibling method
        `create_pipeline` is more ergonomic.

        Params:

        * `req`: A `CreatePipelineRequest` object.
        &#34;&#34;&#34;
        return self._req(Service.PPS, &#34;CreatePipeline&#34;, req=req)

    def create_tf_job_pipeline(
        self,
        pipeline_name,
        tf_job,
        parallelism_spec=None,
        reprocess_spec=None,
        egress=None,
        update=None,
        output_branch=None,
        scale_down_threshold=None,
        resource_requests=None,
        resource_limits=None,
        input=None,
        description=None,
        cache_size=None,
        enable_stats=None,
        reprocess=None,
        max_queue_size=None,
        service=None,
        chunk_spec=None,
        datum_timeout=None,
        job_timeout=None,
        salt=None,
        standby=None,
        datum_tries=None,
        scheduling_spec=None,
        pod_patch=None,
        spout=None,
        spec_commit=None,
    ):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `tf_job`: A `TFJob` object. Pachyderm uses this to create TFJobs
        when running in a Kubernetes cluster on which kubeflow has been
        installed.
        * `parallelism_spec`: An optional `ParallelismSpec` object.
        * `egress`: An optional `Egress` object.
        * `update`: An optional bool specifying whether this should behave as
        an upsert.
        * `output_branch`: An optional string representing the branch to output
        results on.
        * `scale_down_threshold`: An optional `Duration` object.
        * `resource_requests`: An optional `ResourceSpec` object.
        * `resource_limits`: An optional `ResourceSpec` object.
        * `input`: An optional `Input` object.
        * `description`: An optional string describing the pipeline.
        * `cache_size`: An optional string.
        * `enable_stats`: An optional bool.
        * `reprocess`: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * `max_queue_size`: An optional int.
        * `service`: An optional `Service` object.
        * `chunk_spec`: An optional `ChunkSpec` object.
        * `datum_timeout`: An optional `Duration` object.
        * `job_timeout`: An optional `Duration` object.
        * `salt`: An optional string.
        * `standby`: An optional bool.
        * `datum_tries`: An optional int.
        * `scheduling_spec`: An optional `SchedulingSpec` object.
        * `pod_patch`: An optional string.
        * `spout`: An optional `Spout` object.
        * `spec_commit`: An optional `Commit` object.
        * `reprocess_spec`: An optional string specifying how to handle
        already-processed data
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;CreatePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            tf_job=tf_job,
            parallelism_spec=parallelism_spec,
            egress=egress,
            update=update,
            output_branch=output_branch,
            scale_down_threshold=scale_down_threshold,
            resource_requests=resource_requests,
            resource_limits=resource_limits,
            input=input,
            description=description,
            cache_size=cache_size,
            enable_stats=enable_stats,
            reprocess=reprocess,
            max_queue_size=max_queue_size,
            service=service,
            chunk_spec=chunk_spec,
            datum_timeout=datum_timeout,
            job_timeout=job_timeout,
            salt=salt,
            standby=standby,
            datum_tries=datum_tries,
            scheduling_spec=scheduling_spec,
            pod_patch=pod_patch,
            spout=spout,
            spec_commit=spec_commit,
            reprocess_spec=reprocess_spec,
        )

    def inspect_pipeline(self, pipeline_name, history=None):
        &#34;&#34;&#34;
        Inspects a pipeline. Returns a `PipelineInfo` object.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `history`: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;
        pipeline = pps_proto.Pipeline(name=pipeline_name)

        if history is None:
            return self._req(Service.PPS, &#34;InspectPipeline&#34;, pipeline=pipeline)
        else:
            # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
            # with a pipeline filter does, so we use that here
            pipelines = self._req(
                Service.PPS, &#34;ListPipeline&#34;, pipeline=pipeline, history=history
            ).pipeline_info
            assert len(pipelines) &lt;= 1
            return pipelines[0] if len(pipelines) else None

    def list_pipeline(self, history=None, allow_incomplete=None, jqFilter=None):
        &#34;&#34;&#34;
        Lists pipelines. Returns a `PipelineInfos` object.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `history`: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        * `allow_incomplete`: An optional boolean that, if set to `True`, causes
          `list_pipeline` to return PipelineInfos with incomplete data where the
          pipeline spec cannot beretrieved. Incomplete PipelineInfos will have a
          nil Transform field, but will have the fields present in
          EtcdPipelineInfo.
        * `jqFilter`: An optional string containing a `jq` filter that can
          restrict the list of jobs returned, for convenience
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;ListPipeline&#34;,
            history=history,
            allow_incomplete=allow_incomplete,
            jqFilter=jqFilter,
        )

    def delete_pipeline(self, pipeline_name, force=None, keep_repo=None):
        &#34;&#34;&#34;
        Deletes a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `force`: Whether to force delete.
        * `keep_repo`: Whether to keep the repo.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;DeletePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            force=force,
            keep_repo=keep_repo,
        )

    def delete_all_pipelines(self, force=None):
        &#34;&#34;&#34;
        Deletes all pipelines.

        Params:

        * `force`: Whether to force delete.
        &#34;&#34;&#34;
        return self._req(Service.PPS, &#34;DeletePipeline&#34;, all=True, force=force)

    def start_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Starts a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;StartPipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
        )

    def stop_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Stops a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS, &#34;StopPipeline&#34;, pipeline=pps_proto.Pipeline(name=pipeline_name)
        )

    def run_pipeline(self, pipeline_name, provenance=None, job_id=None):
        &#34;&#34;&#34;
        Runs a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `provenance`: An optional iterable of `CommitProvenance` objects
        representing the pipeline execution provenance.
        * `job_id`: An optional string specifying a specific job ID to run.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;RunPipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            provenance=provenance,
            job_id=job_id,
        )

    def run_cron(self, pipeline_name):
        &#34;&#34;&#34;
        Explicitly triggers a pipeline with one or more cron inputs to run
        now.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        &#34;&#34;&#34;

        return self._req(
            Service.PPS,
            &#34;RunCron&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
        )

    def create_secret(self, secret_name, data, labels=None, annotations=None):
        &#34;&#34;&#34;
        Creates a new secret.

        Params:

        * `secret_name`: The name of the secret to create.
        * `data`: A dict of string keys -&gt; string or bytestring values to
        store in the secret. Each key must consist of alphanumeric characters,
        `-`, `_` or `.`.
        * `labels`: A dict of string keys -&gt; string values representing the
        kubernetes labels to attach to the secret.
        * `annotations`: A dict representing the kubernetes annotations to
        attach to the secret.
        &#34;&#34;&#34;

        encoded_data = {}
        for k, v in data.items():
            if isinstance(v, str):
                v = v.encode(&#34;utf8&#34;)
            encoded_data[k] = base64.b64encode(v).decode(&#34;utf8&#34;)

        f = json.dumps(
            {
                &#34;kind&#34;: &#34;Secret&#34;,
                &#34;apiVersion&#34;: &#34;v1&#34;,
                &#34;metadata&#34;: {
                    &#34;name&#34;: secret_name,
                    &#34;labels&#34;: labels,
                    &#34;annotations&#34;: annotations,
                },
                &#34;data&#34;: encoded_data,
            }
        ).encode(&#34;utf8&#34;)

        return self._req(Service.PPS, &#34;CreateSecret&#34;, file=f)

    def delete_secret(self, secret_name):
        &#34;&#34;&#34;
        Deletes a new secret.

        Params:

        * `secret_name`: The name of the secret to delete.
        &#34;&#34;&#34;
        secret = pps_proto.Secret(name=secret_name)
        return self._req(Service.PPS, &#34;DeleteSecret&#34;, secret=secret)

    def list_secret(self):
        &#34;&#34;&#34;
        Lists secrets. Returns a list of `SecretInfo` objects.
        &#34;&#34;&#34;

        return self._req(
            Service.PPS,
            &#34;ListSecret&#34;,
            req=pps_proto.google_dot_protobuf_dot_empty__pb2.Empty(),
        ).secret_info

    def inspect_secret(self, secret_name):
        &#34;&#34;&#34;
        Inspects a secret.

        Params:

        * `secret_name`: The name of the secret to inspect.
        &#34;&#34;&#34;
        secret = pps_proto.Secret(name=secret_name)
        return self._req(Service.PPS, &#34;InspectSecret&#34;, secret=secret)

    def delete_all(self):
        &#34;&#34;&#34;
        Deletes everything in pachyderm.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;DeleteAll&#34;,
            req=pps_proto.google_dot_protobuf_dot_empty__pb2.Empty(),
        )

    def get_pipeline_logs(
        self,
        pipeline_name,
        data_filters=None,
        master=None,
        datum=None,
        follow=None,
        tail=None,
        use_loki_backend=None,
        since=None,
    ):
        &#34;&#34;&#34;
        Gets logs for a pipeline. Yields `LogMessage` objects.

        Params:

        * `pipeline_name`: A string representing a pipeline to get
          logs of.
        * `data_filters`: An optional iterable of strings specifying the names
          of input files from which we want processing logs. This may contain
          multiple files, to query pipelines that contain multiple inputs. Each
          filter may be an absolute path of a file within a pps repo, or it may
          be a hash for that file (to search for files at specific versions.)
        * `master`: An optional bool.
        * `datum`: An optional `Datum` object.
        * `follow`: An optional bool specifying whether logs should continue to
          stream forever.
        * `tail`: An optional int. If nonzero, the number of lines from the end
          of the logs to return.  Note: tail applies per container, so you will
          get tail * &lt;number of pods&gt; total lines back.
        * `use_loki_backend`: Whether to use loki as a backend for fetching
          logs. Requires a loki-enabled cluster.
        * `since`: An optional `Duration` object specifying the start time for
          returned logs
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;GetLogs&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            data_filters=data_filters,
            master=master,
            datum=datum,
            follow=follow,
            tail=tail,
            use_loki_backend=use_loki_backend,
            since=since,
        )

    def get_job_logs(
        self,
        job_id,
        data_filters=None,
        datum=None,
        follow=None,
        tail=None,
        use_loki_backend=None,
        since=None,
    ):
        &#34;&#34;&#34;
        Gets logs for a job. Yields `LogMessage` objects.

        Params:

        * `job_id`: A string representing a job to get logs of.
        * `data_filters`: An optional iterable of strings specifying the names
          of input files from which we want processing logs. This may contain
          multiple files, to query pipelines that contain multiple inputs. Each
          filter may be an absolute path of a file within a pps repo, or it may
          be a hash for that file (to search for files at specific versions.)
        * `datum`: An optional `Datum` object.
        * `follow`: An optional bool specifying whether logs should continue to
          stream forever.
        * `tail`: An optional int. If nonzero, the number of lines from the end
          of the logs to return.  Note: tail applies per container, so you will
          get tail * &lt;number of pods&gt; total lines back.
        * `use_loki_backend`: Whether to use loki as a backend for fetching
          logs. Requires a loki-enabled cluster.
        * `since`: An optional `Duration` object specifying the start time for
          returned logs
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;GetLogs&#34;,
            job=pps_proto.Job(id=job_id),
            data_filters=data_filters,
            datum=datum,
            follow=follow,
            tail=tail,
            use_loki_backend=use_loki_backend,
            since=since,
        )


def pipeline_inputs(root):
    if root is None:
        return
    elif root.cross is not None:
        for i in root.cross:
            yield from pipeline_inputs(i)
    elif root.join is not None:
        for i in root.join:
            yield from pipeline_inputs(i)
    elif root.union is not None:
        for i in root.union:
            yield from pipeline_inputs(i)
    yield root</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="python_pachyderm.mixin.pps.pipeline_inputs"><code class="name flex">
<span>def <span class="ident">pipeline_inputs</span></span>(<span>root)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pipeline_inputs(root):
    if root is None:
        return
    elif root.cross is not None:
        for i in root.cross:
            yield from pipeline_inputs(i)
    elif root.join is not None:
        for i in root.join:
            yield from pipeline_inputs(i)
    elif root.union is not None:
        for i in root.union:
            yield from pipeline_inputs(i)
    yield root</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="python_pachyderm.mixin.pps.PPSMixin"><code class="flex name class">
<span>class <span class="ident">PPSMixin</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PPSMixin:
    def inspect_job(self, job_id, block_state=None, output_commit=None, full=None):
        &#34;&#34;&#34;
        Inspects a job with a given ID. Returns a `JobInfo`.

        Params:

        * `job_id`: The ID of the job to inspect.
        * `block_state`: If true, block until the job completes.
        * `output_commit`: An optional tuple, string, or `Commit` object
        representing an output commit to filter on.
        * `full`: If true, include worker status.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;InspectJob&#34;,
            job=pps_proto.Job(id=job_id),
            block_state=block_state,
            output_commit=commit_from(output_commit)
            if output_commit is not None
            else None,
            full=full,
        )

    def list_job(
        self,
        pipeline_name=None,
        input_commit=None,
        output_commit=None,
        history=None,
        full=None,
        jqFilter=None,
    ):
        &#34;&#34;&#34;
        Lists jobs. Yields `JobInfo` objects.

        Params:

        * `pipeline_name`: An optional string representing a pipeline name to
          filter on.
        * `input_commit`: An optional list of tuples, strings, or `Commit`
          objects representing input commits to filter on.
        * `output_commit`: An optional tuple, string, or `Commit` object
          representing an output commit to filter on.
        * `history`: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        * `full`: An optional bool indicating whether the result should
          include all pipeline details in each `JobInfo`, or limited information
          including name and status, but excluding information in the pipeline
          spec. Leaving this `None` (or `False`) can make the call significantly
          faster in clusters with a large number of pipelines and jobs. Note
          that if `input_commit` is set, this field is coerced to `True`.
        * `jqFilter`: An optional string containing a `jq` filter that can
          restrict the list of jobs returned, for convenience
        &#34;&#34;&#34;
        if isinstance(input_commit, list):
            input_commit = [commit_from(ic) for ic in input_commit]
        elif input_commit is not None:
            input_commit = [commit_from(input_commit)]

        return self._req(
            Service.PPS,
            &#34;ListJob&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name)
            if pipeline_name is not None
            else None,
            input_commit=input_commit,
            output_commit=commit_from(output_commit)
            if output_commit is not None
            else None,
            history=history,
            full=full,
            jqFilter=jqFilter,
        )

    def flush_job(self, commits, pipeline_names=None):
        &#34;&#34;&#34;
        Blocks until all of the jobs which have a set of commits as
        provenance have finished. Yields `JobInfo` objects.

        Params:

        * `commits`: A list of tuples, strings, or `Commit` objects
        representing the commits to flush.
        * `pipeline_names`: An optional list of strings specifying pipeline
        names. If specified, only jobs within these pipelines will be flushed.
        &#34;&#34;&#34;
        if pipeline_names is not None:
            to_pipelines = [pps_proto.Pipeline(name=name) for name in pipeline_names]
        else:
            to_pipelines = None

        return self._req(
            Service.PPS,
            &#34;FlushJob&#34;,
            commits=[commit_from(c) for c in commits],
            to_pipelines=to_pipelines,
        )

    def delete_job(self, job_id):
        &#34;&#34;&#34;
        Deletes a job by its ID.

        Params:

        * `job_id`: The ID of the job to delete.
        &#34;&#34;&#34;
        return self._req(Service.PPS, &#34;DeleteJob&#34;, job=pps_proto.Job(id=job_id))

    def stop_job(self, job_id, output_commit=None):
        &#34;&#34;&#34;
        Stops a job by its ID.

        Params:

        * `job_id`: The ID of the job to stop.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;StopJob&#34;,
            job=pps_proto.Job(id=job_id),
            output_commit=commit_from(output_commit),
        )

    def inspect_datum(self, job_id, datum_id):
        &#34;&#34;&#34;
        Inspects a datum. Returns a `DatumInfo` object.

        Params:

        * `job_id`: The ID of the job.
        * `datum_id`: The ID of the datum.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;InspectDatum&#34;,
            datum=pps_proto.Datum(id=datum_id, job=pps_proto.Job(id=job_id)),
        )

    def list_datum(self, job_id=None):
        &#34;&#34;&#34;
        Lists datums. Yields `DatumInfo` objects.

        Params:

        * `job_id`: An optional int specifying the ID of a job. Exactly one of
          `job_id` (real) or `input` (hypothetical) must be set.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;ListDatum&#34;,
            job=pps_proto.Job(id=job_id),
        )

    def restart_datum(self, job_id, data_filters=None):
        &#34;&#34;&#34;
        Restarts a datum.

        Params:

        * `job_id`: The ID of the job.
        * `data_filters`: An optional iterable of strings.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;RestartDatum&#34;,
            job=pps_proto.Job(id=job_id),
            data_filters=data_filters,
        )

    def create_pipeline(
        self,
        pipeline_name,
        transform,
        parallelism_spec=None,
        egress=None,
        reprocess_spec=None,
        update=None,
        output_branch=None,
        resource_requests=None,
        resource_limits=None,
        input=None,
        description=None,
        cache_size=None,
        enable_stats=None,
        reprocess=None,
        max_queue_size=None,
        service=None,
        chunk_spec=None,
        datum_timeout=None,
        job_timeout=None,
        salt=None,
        standby=None,
        datum_tries=None,
        scheduling_spec=None,
        pod_patch=None,
        spout=None,
        spec_commit=None,
        metadata=None,
        s3_out=None,
        sidecar_resource_limits=None,
    ):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `transform`: A `Transform` object.
        * `parallelism_spec`: An optional `ParallelismSpec` object.
        * `egress`: An optional `Egress` object.
        * `update`: An optional bool specifying whether this should behave as
        an upsert.
        * `output_branch`: An optional string representing the branch to output
        results on.
        * `resource_requests`: An optional `ResourceSpec` object.
        * `resource_limits`: An optional `ResourceSpec` object.
        * `input`: An optional `Input` object.
        * `description`: An optional string describing the pipeline.
        * `cache_size`: An optional string.
        * `enable_stats`: An optional bool.
        * `reprocess`: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * `max_queue_size`: An optional int.
        * `service`: An optional `Service` object.
        * `chunk_spec`: An optional `ChunkSpec` object.
        * `datum_timeout`: An optional `Duration` object.
        * `job_timeout`: An optional `Duration` object.
        * `salt`: An optional string.
        * `standby`: An optional bool.
        * `datum_tries`: An optional int.
        * `scheduling_spec`: An optional `SchedulingSpec` object.
        * `pod_patch`: An optional string.
        * `spout`: An optional `Spout` object.
        * `spec_commit`: An optional `Commit` object.
        * `metadata`: An optional `Metadata` object.
        * `s3_out`: An optional bool specifying whether the output repo should
        be exposed as an s3 gateway bucket.
        * `sidecar_resource_limits`: An optional `ResourceSpec` setting
        * `reprocess_spec`: An optional string specifying how to handle
        already-processed data
        resource limits for the pipeline sidecar.
        &#34;&#34;&#34;

        # Support for build step-enabled pipelines. This is a python port of
        # the equivalent functionality in pachyderm core&#39;s
        # &#39;src/server/pps/cmds/cmds.go&#39;, and any changes made here likely have
        # to be reflected there as well.
        if transform.build.image or transform.build.language or transform.build.path:
            if spout:
                raise Exception(&#34;build step-enabled pipelines do not work with spouts&#34;)
            if not input:
                raise Exception(&#34;no `input` specified&#34;)
            if (not transform.build.language) and (not transform.build.image):
                raise Exception(&#34;must specify either a build `language` or `image`&#34;)
            if transform.build.language and transform.build.image:
                raise Exception(&#34;cannot specify both a build `language` and `image`&#34;)
            if any(
                i.pfs is not None and i.pfs.name in (&#34;build&#34;, &#34;source&#34;)
                for i in pipeline_inputs(input)
            ):
                raise Exception(
                    &#34;build step-enabled pipelines cannot have inputs with the name &#34;
                    + &#34;&#39;build&#39; or &#39;source&#39;, as they are reserved for build assets&#34;
                )

            build_path = Path(transform.build.path or &#34;.&#34;)
            if not build_path.exists():
                raise Exception(&#34;build path {} does not exist&#34;.format(build_path))
            if (build_path / &#34;.pachignore&#34;).exists():
                warnings.warn(
                    &#34;detected a &#39;.pachignore&#39; file, but it&#39;s unsupported by python_pachyderm -- use `pachctl` instead&#34;,
                    RuntimeWarning,
                )

            build_pipeline_name = &#34;{}_build&#34;.format(pipeline_name)

            image = transform.build.image
            if not image:
                version = self.get_remote_version()
                version_str = &#34;{}.{}.{}{}&#34;.format(
                    version.major, version.minor, version.micro, version.additional
                )
                image = &#34;pachyderm/{}-build:{}&#34;.format(
                    transform.build.language, version_str
                )
            if not transform.image:
                transform.image = image

            def create_build_pipeline_input(name):
                return pps_proto.Input(
                    pfs=pps_proto.PFSInput(
                        name=name,
                        glob=&#34;/&#34;,
                        repo=build_pipeline_name,
                        branch=name,
                    )
                )

            self.create_repo(build_pipeline_name, update=True)

            self._req(
                Service.PPS,
                &#34;CreatePipeline&#34;,
                pipeline=pps_proto.Pipeline(name=build_pipeline_name),
                transform=pps_proto.Transform(image=image, cmd=[&#34;sh&#34;, &#34;./build.sh&#34;]),
                parallelism_spec=pps_proto.ParallelismSpec(constant=1),
                input=create_build_pipeline_input(&#34;source&#34;),
                output_branch=&#34;build&#34;,
                update=update,
            )

            with self.modify_file_client((build_pipeline_name, &#34;source&#34;)) as pfc:
                if update:
                    pfc.delete_file(&#34;/&#34;)
                for root, _, filenames in os.walk(str(build_path)):
                    for filename in filenames:
                        source_filepath = os.path.join(root, filename)
                        dest_filepath = os.path.join(
                            &#34;/&#34;, os.path.relpath(source_filepath, start=str(build_path))
                        )
                        pfc.put_file_from_filepath(dest_filepath, source_filepath)

            input = pps_proto.Input(
                cross=[
                    create_build_pipeline_input(&#34;source&#34;),
                    create_build_pipeline_input(&#34;build&#34;),
                    input,
                ]
            )

            if not transform.cmd:
                transform.cmd[:] = [&#34;sh&#34;, &#34;/pfs/build/run.sh&#34;]

        return self._req(
            Service.PPS,
            &#34;CreatePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            transform=transform,
            parallelism_spec=parallelism_spec,
            egress=egress,
            update=update,
            output_branch=output_branch,
            resource_requests=resource_requests,
            resource_limits=resource_limits,
            input=input,
            description=description,
            cache_size=cache_size,
            enable_stats=enable_stats,
            reprocess=reprocess,
            max_queue_size=max_queue_size,
            metadata=metadata,
            service=service,
            chunk_spec=chunk_spec,
            datum_timeout=datum_timeout,
            job_timeout=job_timeout,
            salt=salt,
            standby=standby,
            datum_tries=datum_tries,
            scheduling_spec=scheduling_spec,
            pod_patch=pod_patch,
            spout=spout,
            spec_commit=spec_commit,
            sidecar_resource_limits=sidecar_resource_limits,
            reprocess_spec=reprocess_spec,
        )

    def create_pipeline_from_request(self, req):
        &#34;&#34;&#34;
        Creates a pipeline from a `CreatePipelineRequest` object. Usually this
        would be used in conjunction with `util.parse_json_pipeline_spec` or
        `util.parse_dict_pipeline_spec`. If you&#39;re in pure python and not
        working with a pipeline spec file, the sibling method
        `create_pipeline` is more ergonomic.

        Params:

        * `req`: A `CreatePipelineRequest` object.
        &#34;&#34;&#34;
        return self._req(Service.PPS, &#34;CreatePipeline&#34;, req=req)

    def create_tf_job_pipeline(
        self,
        pipeline_name,
        tf_job,
        parallelism_spec=None,
        reprocess_spec=None,
        egress=None,
        update=None,
        output_branch=None,
        scale_down_threshold=None,
        resource_requests=None,
        resource_limits=None,
        input=None,
        description=None,
        cache_size=None,
        enable_stats=None,
        reprocess=None,
        max_queue_size=None,
        service=None,
        chunk_spec=None,
        datum_timeout=None,
        job_timeout=None,
        salt=None,
        standby=None,
        datum_tries=None,
        scheduling_spec=None,
        pod_patch=None,
        spout=None,
        spec_commit=None,
    ):
        &#34;&#34;&#34;
        Creates a pipeline. For more info, please refer to the pipeline spec
        document:
        http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `tf_job`: A `TFJob` object. Pachyderm uses this to create TFJobs
        when running in a Kubernetes cluster on which kubeflow has been
        installed.
        * `parallelism_spec`: An optional `ParallelismSpec` object.
        * `egress`: An optional `Egress` object.
        * `update`: An optional bool specifying whether this should behave as
        an upsert.
        * `output_branch`: An optional string representing the branch to output
        results on.
        * `scale_down_threshold`: An optional `Duration` object.
        * `resource_requests`: An optional `ResourceSpec` object.
        * `resource_limits`: An optional `ResourceSpec` object.
        * `input`: An optional `Input` object.
        * `description`: An optional string describing the pipeline.
        * `cache_size`: An optional string.
        * `enable_stats`: An optional bool.
        * `reprocess`: An optional bool. If true, pachyderm forces the pipeline
        to reprocess all datums. It only has meaning if `update` is `True`.
        * `max_queue_size`: An optional int.
        * `service`: An optional `Service` object.
        * `chunk_spec`: An optional `ChunkSpec` object.
        * `datum_timeout`: An optional `Duration` object.
        * `job_timeout`: An optional `Duration` object.
        * `salt`: An optional string.
        * `standby`: An optional bool.
        * `datum_tries`: An optional int.
        * `scheduling_spec`: An optional `SchedulingSpec` object.
        * `pod_patch`: An optional string.
        * `spout`: An optional `Spout` object.
        * `spec_commit`: An optional `Commit` object.
        * `reprocess_spec`: An optional string specifying how to handle
        already-processed data
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;CreatePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            tf_job=tf_job,
            parallelism_spec=parallelism_spec,
            egress=egress,
            update=update,
            output_branch=output_branch,
            scale_down_threshold=scale_down_threshold,
            resource_requests=resource_requests,
            resource_limits=resource_limits,
            input=input,
            description=description,
            cache_size=cache_size,
            enable_stats=enable_stats,
            reprocess=reprocess,
            max_queue_size=max_queue_size,
            service=service,
            chunk_spec=chunk_spec,
            datum_timeout=datum_timeout,
            job_timeout=job_timeout,
            salt=salt,
            standby=standby,
            datum_tries=datum_tries,
            scheduling_spec=scheduling_spec,
            pod_patch=pod_patch,
            spout=spout,
            spec_commit=spec_commit,
            reprocess_spec=reprocess_spec,
        )

    def inspect_pipeline(self, pipeline_name, history=None):
        &#34;&#34;&#34;
        Inspects a pipeline. Returns a `PipelineInfo` object.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `history`: An optional int that indicates to return jobs from
        historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        &#34;&#34;&#34;
        pipeline = pps_proto.Pipeline(name=pipeline_name)

        if history is None:
            return self._req(Service.PPS, &#34;InspectPipeline&#34;, pipeline=pipeline)
        else:
            # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
            # with a pipeline filter does, so we use that here
            pipelines = self._req(
                Service.PPS, &#34;ListPipeline&#34;, pipeline=pipeline, history=history
            ).pipeline_info
            assert len(pipelines) &lt;= 1
            return pipelines[0] if len(pipelines) else None

    def list_pipeline(self, history=None, allow_incomplete=None, jqFilter=None):
        &#34;&#34;&#34;
        Lists pipelines. Returns a `PipelineInfos` object.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `history`: An optional int that indicates to return jobs from
          historical versions of pipelines. Semantics are:
            * 0: Return jobs from the current version of the pipeline or
              pipelines.
            * 1: Return the above and jobs from the next most recent version
            * 2: etc.
            * -1: Return jobs from all historical versions.
        * `allow_incomplete`: An optional boolean that, if set to `True`, causes
          `list_pipeline` to return PipelineInfos with incomplete data where the
          pipeline spec cannot beretrieved. Incomplete PipelineInfos will have a
          nil Transform field, but will have the fields present in
          EtcdPipelineInfo.
        * `jqFilter`: An optional string containing a `jq` filter that can
          restrict the list of jobs returned, for convenience
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;ListPipeline&#34;,
            history=history,
            allow_incomplete=allow_incomplete,
            jqFilter=jqFilter,
        )

    def delete_pipeline(self, pipeline_name, force=None, keep_repo=None):
        &#34;&#34;&#34;
        Deletes a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `force`: Whether to force delete.
        * `keep_repo`: Whether to keep the repo.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;DeletePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            force=force,
            keep_repo=keep_repo,
        )

    def delete_all_pipelines(self, force=None):
        &#34;&#34;&#34;
        Deletes all pipelines.

        Params:

        * `force`: Whether to force delete.
        &#34;&#34;&#34;
        return self._req(Service.PPS, &#34;DeletePipeline&#34;, all=True, force=force)

    def start_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Starts a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;StartPipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
        )

    def stop_pipeline(self, pipeline_name):
        &#34;&#34;&#34;
        Stops a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS, &#34;StopPipeline&#34;, pipeline=pps_proto.Pipeline(name=pipeline_name)
        )

    def run_pipeline(self, pipeline_name, provenance=None, job_id=None):
        &#34;&#34;&#34;
        Runs a pipeline.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        * `provenance`: An optional iterable of `CommitProvenance` objects
        representing the pipeline execution provenance.
        * `job_id`: An optional string specifying a specific job ID to run.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;RunPipeline&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            provenance=provenance,
            job_id=job_id,
        )

    def run_cron(self, pipeline_name):
        &#34;&#34;&#34;
        Explicitly triggers a pipeline with one or more cron inputs to run
        now.

        Params:

        * `pipeline_name`: A string representing the pipeline name.
        &#34;&#34;&#34;

        return self._req(
            Service.PPS,
            &#34;RunCron&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
        )

    def create_secret(self, secret_name, data, labels=None, annotations=None):
        &#34;&#34;&#34;
        Creates a new secret.

        Params:

        * `secret_name`: The name of the secret to create.
        * `data`: A dict of string keys -&gt; string or bytestring values to
        store in the secret. Each key must consist of alphanumeric characters,
        `-`, `_` or `.`.
        * `labels`: A dict of string keys -&gt; string values representing the
        kubernetes labels to attach to the secret.
        * `annotations`: A dict representing the kubernetes annotations to
        attach to the secret.
        &#34;&#34;&#34;

        encoded_data = {}
        for k, v in data.items():
            if isinstance(v, str):
                v = v.encode(&#34;utf8&#34;)
            encoded_data[k] = base64.b64encode(v).decode(&#34;utf8&#34;)

        f = json.dumps(
            {
                &#34;kind&#34;: &#34;Secret&#34;,
                &#34;apiVersion&#34;: &#34;v1&#34;,
                &#34;metadata&#34;: {
                    &#34;name&#34;: secret_name,
                    &#34;labels&#34;: labels,
                    &#34;annotations&#34;: annotations,
                },
                &#34;data&#34;: encoded_data,
            }
        ).encode(&#34;utf8&#34;)

        return self._req(Service.PPS, &#34;CreateSecret&#34;, file=f)

    def delete_secret(self, secret_name):
        &#34;&#34;&#34;
        Deletes a new secret.

        Params:

        * `secret_name`: The name of the secret to delete.
        &#34;&#34;&#34;
        secret = pps_proto.Secret(name=secret_name)
        return self._req(Service.PPS, &#34;DeleteSecret&#34;, secret=secret)

    def list_secret(self):
        &#34;&#34;&#34;
        Lists secrets. Returns a list of `SecretInfo` objects.
        &#34;&#34;&#34;

        return self._req(
            Service.PPS,
            &#34;ListSecret&#34;,
            req=pps_proto.google_dot_protobuf_dot_empty__pb2.Empty(),
        ).secret_info

    def inspect_secret(self, secret_name):
        &#34;&#34;&#34;
        Inspects a secret.

        Params:

        * `secret_name`: The name of the secret to inspect.
        &#34;&#34;&#34;
        secret = pps_proto.Secret(name=secret_name)
        return self._req(Service.PPS, &#34;InspectSecret&#34;, secret=secret)

    def delete_all(self):
        &#34;&#34;&#34;
        Deletes everything in pachyderm.
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;DeleteAll&#34;,
            req=pps_proto.google_dot_protobuf_dot_empty__pb2.Empty(),
        )

    def get_pipeline_logs(
        self,
        pipeline_name,
        data_filters=None,
        master=None,
        datum=None,
        follow=None,
        tail=None,
        use_loki_backend=None,
        since=None,
    ):
        &#34;&#34;&#34;
        Gets logs for a pipeline. Yields `LogMessage` objects.

        Params:

        * `pipeline_name`: A string representing a pipeline to get
          logs of.
        * `data_filters`: An optional iterable of strings specifying the names
          of input files from which we want processing logs. This may contain
          multiple files, to query pipelines that contain multiple inputs. Each
          filter may be an absolute path of a file within a pps repo, or it may
          be a hash for that file (to search for files at specific versions.)
        * `master`: An optional bool.
        * `datum`: An optional `Datum` object.
        * `follow`: An optional bool specifying whether logs should continue to
          stream forever.
        * `tail`: An optional int. If nonzero, the number of lines from the end
          of the logs to return.  Note: tail applies per container, so you will
          get tail * &lt;number of pods&gt; total lines back.
        * `use_loki_backend`: Whether to use loki as a backend for fetching
          logs. Requires a loki-enabled cluster.
        * `since`: An optional `Duration` object specifying the start time for
          returned logs
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;GetLogs&#34;,
            pipeline=pps_proto.Pipeline(name=pipeline_name),
            data_filters=data_filters,
            master=master,
            datum=datum,
            follow=follow,
            tail=tail,
            use_loki_backend=use_loki_backend,
            since=since,
        )

    def get_job_logs(
        self,
        job_id,
        data_filters=None,
        datum=None,
        follow=None,
        tail=None,
        use_loki_backend=None,
        since=None,
    ):
        &#34;&#34;&#34;
        Gets logs for a job. Yields `LogMessage` objects.

        Params:

        * `job_id`: A string representing a job to get logs of.
        * `data_filters`: An optional iterable of strings specifying the names
          of input files from which we want processing logs. This may contain
          multiple files, to query pipelines that contain multiple inputs. Each
          filter may be an absolute path of a file within a pps repo, or it may
          be a hash for that file (to search for files at specific versions.)
        * `datum`: An optional `Datum` object.
        * `follow`: An optional bool specifying whether logs should continue to
          stream forever.
        * `tail`: An optional int. If nonzero, the number of lines from the end
          of the logs to return.  Note: tail applies per container, so you will
          get tail * &lt;number of pods&gt; total lines back.
        * `use_loki_backend`: Whether to use loki as a backend for fetching
          logs. Requires a loki-enabled cluster.
        * `since`: An optional `Duration` object specifying the start time for
          returned logs
        &#34;&#34;&#34;
        return self._req(
            Service.PPS,
            &#34;GetLogs&#34;,
            job=pps_proto.Job(id=job_id),
            data_filters=data_filters,
            datum=datum,
            follow=follow,
            tail=tail,
            use_loki_backend=use_loki_backend,
            since=since,
        )</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="python_pachyderm.client.Client" href="../client.html#python_pachyderm.client.Client">Client</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="python_pachyderm.mixin.pps.PPSMixin.create_pipeline"><code class="name flex">
<span>def <span class="ident">create_pipeline</span></span>(<span>self, pipeline_name, transform, parallelism_spec=None, egress=None, reprocess_spec=None, update=None, output_branch=None, resource_requests=None, resource_limits=None, input=None, description=None, cache_size=None, enable_stats=None, reprocess=None, max_queue_size=None, service=None, chunk_spec=None, datum_timeout=None, job_timeout=None, salt=None, standby=None, datum_tries=None, scheduling_spec=None, pod_patch=None, spout=None, spec_commit=None, metadata=None, s3_out=None, sidecar_resource_limits=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a pipeline. For more info, please refer to the pipeline spec
document:
<a href="http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html">http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html</a></p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
<li><code>transform</code>: A <code>Transform</code> object.</li>
<li><code>parallelism_spec</code>: An optional <code>ParallelismSpec</code> object.</li>
<li><code>egress</code>: An optional <code>Egress</code> object.</li>
<li><code>update</code>: An optional bool specifying whether this should behave as
an upsert.</li>
<li><code>output_branch</code>: An optional string representing the branch to output
results on.</li>
<li><code>resource_requests</code>: An optional <code>ResourceSpec</code> object.</li>
<li><code>resource_limits</code>: An optional <code>ResourceSpec</code> object.</li>
<li><code>input</code>: An optional <code>Input</code> object.</li>
<li><code>description</code>: An optional string describing the pipeline.</li>
<li><code>cache_size</code>: An optional string.</li>
<li><code>enable_stats</code>: An optional bool.</li>
<li><code>reprocess</code>: An optional bool. If true, pachyderm forces the pipeline
to reprocess all datums. It only has meaning if <code>update</code> is <code>True</code>.</li>
<li><code>max_queue_size</code>: An optional int.</li>
<li><code>service</code>: An optional <code>Service</code> object.</li>
<li><code>chunk_spec</code>: An optional <code>ChunkSpec</code> object.</li>
<li><code>datum_timeout</code>: An optional <code>Duration</code> object.</li>
<li><code>job_timeout</code>: An optional <code>Duration</code> object.</li>
<li><code>salt</code>: An optional string.</li>
<li><code>standby</code>: An optional bool.</li>
<li><code>datum_tries</code>: An optional int.</li>
<li><code>scheduling_spec</code>: An optional <code>SchedulingSpec</code> object.</li>
<li><code>pod_patch</code>: An optional string.</li>
<li><code>spout</code>: An optional <code>Spout</code> object.</li>
<li><code>spec_commit</code>: An optional <code>Commit</code> object.</li>
<li><code>metadata</code>: An optional <code>Metadata</code> object.</li>
<li><code>s3_out</code>: An optional bool specifying whether the output repo should
be exposed as an s3 gateway bucket.</li>
<li><code>sidecar_resource_limits</code>: An optional <code>ResourceSpec</code> setting</li>
<li><code>reprocess_spec</code>: An optional string specifying how to handle
already-processed data
resource limits for the pipeline sidecar.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_pipeline(
    self,
    pipeline_name,
    transform,
    parallelism_spec=None,
    egress=None,
    reprocess_spec=None,
    update=None,
    output_branch=None,
    resource_requests=None,
    resource_limits=None,
    input=None,
    description=None,
    cache_size=None,
    enable_stats=None,
    reprocess=None,
    max_queue_size=None,
    service=None,
    chunk_spec=None,
    datum_timeout=None,
    job_timeout=None,
    salt=None,
    standby=None,
    datum_tries=None,
    scheduling_spec=None,
    pod_patch=None,
    spout=None,
    spec_commit=None,
    metadata=None,
    s3_out=None,
    sidecar_resource_limits=None,
):
    &#34;&#34;&#34;
    Creates a pipeline. For more info, please refer to the pipeline spec
    document:
    http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    * `transform`: A `Transform` object.
    * `parallelism_spec`: An optional `ParallelismSpec` object.
    * `egress`: An optional `Egress` object.
    * `update`: An optional bool specifying whether this should behave as
    an upsert.
    * `output_branch`: An optional string representing the branch to output
    results on.
    * `resource_requests`: An optional `ResourceSpec` object.
    * `resource_limits`: An optional `ResourceSpec` object.
    * `input`: An optional `Input` object.
    * `description`: An optional string describing the pipeline.
    * `cache_size`: An optional string.
    * `enable_stats`: An optional bool.
    * `reprocess`: An optional bool. If true, pachyderm forces the pipeline
    to reprocess all datums. It only has meaning if `update` is `True`.
    * `max_queue_size`: An optional int.
    * `service`: An optional `Service` object.
    * `chunk_spec`: An optional `ChunkSpec` object.
    * `datum_timeout`: An optional `Duration` object.
    * `job_timeout`: An optional `Duration` object.
    * `salt`: An optional string.
    * `standby`: An optional bool.
    * `datum_tries`: An optional int.
    * `scheduling_spec`: An optional `SchedulingSpec` object.
    * `pod_patch`: An optional string.
    * `spout`: An optional `Spout` object.
    * `spec_commit`: An optional `Commit` object.
    * `metadata`: An optional `Metadata` object.
    * `s3_out`: An optional bool specifying whether the output repo should
    be exposed as an s3 gateway bucket.
    * `sidecar_resource_limits`: An optional `ResourceSpec` setting
    * `reprocess_spec`: An optional string specifying how to handle
    already-processed data
    resource limits for the pipeline sidecar.
    &#34;&#34;&#34;

    # Support for build step-enabled pipelines. This is a python port of
    # the equivalent functionality in pachyderm core&#39;s
    # &#39;src/server/pps/cmds/cmds.go&#39;, and any changes made here likely have
    # to be reflected there as well.
    if transform.build.image or transform.build.language or transform.build.path:
        if spout:
            raise Exception(&#34;build step-enabled pipelines do not work with spouts&#34;)
        if not input:
            raise Exception(&#34;no `input` specified&#34;)
        if (not transform.build.language) and (not transform.build.image):
            raise Exception(&#34;must specify either a build `language` or `image`&#34;)
        if transform.build.language and transform.build.image:
            raise Exception(&#34;cannot specify both a build `language` and `image`&#34;)
        if any(
            i.pfs is not None and i.pfs.name in (&#34;build&#34;, &#34;source&#34;)
            for i in pipeline_inputs(input)
        ):
            raise Exception(
                &#34;build step-enabled pipelines cannot have inputs with the name &#34;
                + &#34;&#39;build&#39; or &#39;source&#39;, as they are reserved for build assets&#34;
            )

        build_path = Path(transform.build.path or &#34;.&#34;)
        if not build_path.exists():
            raise Exception(&#34;build path {} does not exist&#34;.format(build_path))
        if (build_path / &#34;.pachignore&#34;).exists():
            warnings.warn(
                &#34;detected a &#39;.pachignore&#39; file, but it&#39;s unsupported by python_pachyderm -- use `pachctl` instead&#34;,
                RuntimeWarning,
            )

        build_pipeline_name = &#34;{}_build&#34;.format(pipeline_name)

        image = transform.build.image
        if not image:
            version = self.get_remote_version()
            version_str = &#34;{}.{}.{}{}&#34;.format(
                version.major, version.minor, version.micro, version.additional
            )
            image = &#34;pachyderm/{}-build:{}&#34;.format(
                transform.build.language, version_str
            )
        if not transform.image:
            transform.image = image

        def create_build_pipeline_input(name):
            return pps_proto.Input(
                pfs=pps_proto.PFSInput(
                    name=name,
                    glob=&#34;/&#34;,
                    repo=build_pipeline_name,
                    branch=name,
                )
            )

        self.create_repo(build_pipeline_name, update=True)

        self._req(
            Service.PPS,
            &#34;CreatePipeline&#34;,
            pipeline=pps_proto.Pipeline(name=build_pipeline_name),
            transform=pps_proto.Transform(image=image, cmd=[&#34;sh&#34;, &#34;./build.sh&#34;]),
            parallelism_spec=pps_proto.ParallelismSpec(constant=1),
            input=create_build_pipeline_input(&#34;source&#34;),
            output_branch=&#34;build&#34;,
            update=update,
        )

        with self.modify_file_client((build_pipeline_name, &#34;source&#34;)) as pfc:
            if update:
                pfc.delete_file(&#34;/&#34;)
            for root, _, filenames in os.walk(str(build_path)):
                for filename in filenames:
                    source_filepath = os.path.join(root, filename)
                    dest_filepath = os.path.join(
                        &#34;/&#34;, os.path.relpath(source_filepath, start=str(build_path))
                    )
                    pfc.put_file_from_filepath(dest_filepath, source_filepath)

        input = pps_proto.Input(
            cross=[
                create_build_pipeline_input(&#34;source&#34;),
                create_build_pipeline_input(&#34;build&#34;),
                input,
            ]
        )

        if not transform.cmd:
            transform.cmd[:] = [&#34;sh&#34;, &#34;/pfs/build/run.sh&#34;]

    return self._req(
        Service.PPS,
        &#34;CreatePipeline&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        transform=transform,
        parallelism_spec=parallelism_spec,
        egress=egress,
        update=update,
        output_branch=output_branch,
        resource_requests=resource_requests,
        resource_limits=resource_limits,
        input=input,
        description=description,
        cache_size=cache_size,
        enable_stats=enable_stats,
        reprocess=reprocess,
        max_queue_size=max_queue_size,
        metadata=metadata,
        service=service,
        chunk_spec=chunk_spec,
        datum_timeout=datum_timeout,
        job_timeout=job_timeout,
        salt=salt,
        standby=standby,
        datum_tries=datum_tries,
        scheduling_spec=scheduling_spec,
        pod_patch=pod_patch,
        spout=spout,
        spec_commit=spec_commit,
        sidecar_resource_limits=sidecar_resource_limits,
        reprocess_spec=reprocess_spec,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.create_pipeline_from_request"><code class="name flex">
<span>def <span class="ident">create_pipeline_from_request</span></span>(<span>self, req)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a pipeline from a <code>CreatePipelineRequest</code> object. Usually this
would be used in conjunction with <code>util.parse_json_pipeline_spec</code> or
<code>util.parse_dict_pipeline_spec</code>. If you're in pure python and not
working with a pipeline spec file, the sibling method
<code>create_pipeline</code> is more ergonomic.</p>
<p>Params:</p>
<ul>
<li><code>req</code>: A <code>CreatePipelineRequest</code> object.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_pipeline_from_request(self, req):
    &#34;&#34;&#34;
    Creates a pipeline from a `CreatePipelineRequest` object. Usually this
    would be used in conjunction with `util.parse_json_pipeline_spec` or
    `util.parse_dict_pipeline_spec`. If you&#39;re in pure python and not
    working with a pipeline spec file, the sibling method
    `create_pipeline` is more ergonomic.

    Params:

    * `req`: A `CreatePipelineRequest` object.
    &#34;&#34;&#34;
    return self._req(Service.PPS, &#34;CreatePipeline&#34;, req=req)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.create_secret"><code class="name flex">
<span>def <span class="ident">create_secret</span></span>(<span>self, secret_name, data, labels=None, annotations=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new secret.</p>
<p>Params:</p>
<ul>
<li><code>secret_name</code>: The name of the secret to create.</li>
<li><code>data</code>: A dict of string keys -&gt; string or bytestring values to
store in the secret. Each key must consist of alphanumeric characters,
<code>-</code>, <code>_</code> or <code>.</code>.</li>
<li><code>labels</code>: A dict of string keys -&gt; string values representing the
kubernetes labels to attach to the secret.</li>
<li><code>annotations</code>: A dict representing the kubernetes annotations to
attach to the secret.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_secret(self, secret_name, data, labels=None, annotations=None):
    &#34;&#34;&#34;
    Creates a new secret.

    Params:

    * `secret_name`: The name of the secret to create.
    * `data`: A dict of string keys -&gt; string or bytestring values to
    store in the secret. Each key must consist of alphanumeric characters,
    `-`, `_` or `.`.
    * `labels`: A dict of string keys -&gt; string values representing the
    kubernetes labels to attach to the secret.
    * `annotations`: A dict representing the kubernetes annotations to
    attach to the secret.
    &#34;&#34;&#34;

    encoded_data = {}
    for k, v in data.items():
        if isinstance(v, str):
            v = v.encode(&#34;utf8&#34;)
        encoded_data[k] = base64.b64encode(v).decode(&#34;utf8&#34;)

    f = json.dumps(
        {
            &#34;kind&#34;: &#34;Secret&#34;,
            &#34;apiVersion&#34;: &#34;v1&#34;,
            &#34;metadata&#34;: {
                &#34;name&#34;: secret_name,
                &#34;labels&#34;: labels,
                &#34;annotations&#34;: annotations,
            },
            &#34;data&#34;: encoded_data,
        }
    ).encode(&#34;utf8&#34;)

    return self._req(Service.PPS, &#34;CreateSecret&#34;, file=f)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.create_tf_job_pipeline"><code class="name flex">
<span>def <span class="ident">create_tf_job_pipeline</span></span>(<span>self, pipeline_name, tf_job, parallelism_spec=None, reprocess_spec=None, egress=None, update=None, output_branch=None, scale_down_threshold=None, resource_requests=None, resource_limits=None, input=None, description=None, cache_size=None, enable_stats=None, reprocess=None, max_queue_size=None, service=None, chunk_spec=None, datum_timeout=None, job_timeout=None, salt=None, standby=None, datum_tries=None, scheduling_spec=None, pod_patch=None, spout=None, spec_commit=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a pipeline. For more info, please refer to the pipeline spec
document:
<a href="http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html">http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html</a></p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
<li><code>tf_job</code>: A <code>TFJob</code> object. Pachyderm uses this to create TFJobs
when running in a Kubernetes cluster on which kubeflow has been
installed.</li>
<li><code>parallelism_spec</code>: An optional <code>ParallelismSpec</code> object.</li>
<li><code>egress</code>: An optional <code>Egress</code> object.</li>
<li><code>update</code>: An optional bool specifying whether this should behave as
an upsert.</li>
<li><code>output_branch</code>: An optional string representing the branch to output
results on.</li>
<li><code>scale_down_threshold</code>: An optional <code>Duration</code> object.</li>
<li><code>resource_requests</code>: An optional <code>ResourceSpec</code> object.</li>
<li><code>resource_limits</code>: An optional <code>ResourceSpec</code> object.</li>
<li><code>input</code>: An optional <code>Input</code> object.</li>
<li><code>description</code>: An optional string describing the pipeline.</li>
<li><code>cache_size</code>: An optional string.</li>
<li><code>enable_stats</code>: An optional bool.</li>
<li><code>reprocess</code>: An optional bool. If true, pachyderm forces the pipeline
to reprocess all datums. It only has meaning if <code>update</code> is <code>True</code>.</li>
<li><code>max_queue_size</code>: An optional int.</li>
<li><code>service</code>: An optional <code>Service</code> object.</li>
<li><code>chunk_spec</code>: An optional <code>ChunkSpec</code> object.</li>
<li><code>datum_timeout</code>: An optional <code>Duration</code> object.</li>
<li><code>job_timeout</code>: An optional <code>Duration</code> object.</li>
<li><code>salt</code>: An optional string.</li>
<li><code>standby</code>: An optional bool.</li>
<li><code>datum_tries</code>: An optional int.</li>
<li><code>scheduling_spec</code>: An optional <code>SchedulingSpec</code> object.</li>
<li><code>pod_patch</code>: An optional string.</li>
<li><code>spout</code>: An optional <code>Spout</code> object.</li>
<li><code>spec_commit</code>: An optional <code>Commit</code> object.</li>
<li><code>reprocess_spec</code>: An optional string specifying how to handle
already-processed data</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_tf_job_pipeline(
    self,
    pipeline_name,
    tf_job,
    parallelism_spec=None,
    reprocess_spec=None,
    egress=None,
    update=None,
    output_branch=None,
    scale_down_threshold=None,
    resource_requests=None,
    resource_limits=None,
    input=None,
    description=None,
    cache_size=None,
    enable_stats=None,
    reprocess=None,
    max_queue_size=None,
    service=None,
    chunk_spec=None,
    datum_timeout=None,
    job_timeout=None,
    salt=None,
    standby=None,
    datum_tries=None,
    scheduling_spec=None,
    pod_patch=None,
    spout=None,
    spec_commit=None,
):
    &#34;&#34;&#34;
    Creates a pipeline. For more info, please refer to the pipeline spec
    document:
    http://docs.pachyderm.io/en/latest/reference/pipeline_spec.html

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    * `tf_job`: A `TFJob` object. Pachyderm uses this to create TFJobs
    when running in a Kubernetes cluster on which kubeflow has been
    installed.
    * `parallelism_spec`: An optional `ParallelismSpec` object.
    * `egress`: An optional `Egress` object.
    * `update`: An optional bool specifying whether this should behave as
    an upsert.
    * `output_branch`: An optional string representing the branch to output
    results on.
    * `scale_down_threshold`: An optional `Duration` object.
    * `resource_requests`: An optional `ResourceSpec` object.
    * `resource_limits`: An optional `ResourceSpec` object.
    * `input`: An optional `Input` object.
    * `description`: An optional string describing the pipeline.
    * `cache_size`: An optional string.
    * `enable_stats`: An optional bool.
    * `reprocess`: An optional bool. If true, pachyderm forces the pipeline
    to reprocess all datums. It only has meaning if `update` is `True`.
    * `max_queue_size`: An optional int.
    * `service`: An optional `Service` object.
    * `chunk_spec`: An optional `ChunkSpec` object.
    * `datum_timeout`: An optional `Duration` object.
    * `job_timeout`: An optional `Duration` object.
    * `salt`: An optional string.
    * `standby`: An optional bool.
    * `datum_tries`: An optional int.
    * `scheduling_spec`: An optional `SchedulingSpec` object.
    * `pod_patch`: An optional string.
    * `spout`: An optional `Spout` object.
    * `spec_commit`: An optional `Commit` object.
    * `reprocess_spec`: An optional string specifying how to handle
    already-processed data
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;CreatePipeline&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        tf_job=tf_job,
        parallelism_spec=parallelism_spec,
        egress=egress,
        update=update,
        output_branch=output_branch,
        scale_down_threshold=scale_down_threshold,
        resource_requests=resource_requests,
        resource_limits=resource_limits,
        input=input,
        description=description,
        cache_size=cache_size,
        enable_stats=enable_stats,
        reprocess=reprocess,
        max_queue_size=max_queue_size,
        service=service,
        chunk_spec=chunk_spec,
        datum_timeout=datum_timeout,
        job_timeout=job_timeout,
        salt=salt,
        standby=standby,
        datum_tries=datum_tries,
        scheduling_spec=scheduling_spec,
        pod_patch=pod_patch,
        spout=spout,
        spec_commit=spec_commit,
        reprocess_spec=reprocess_spec,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.delete_all"><code class="name flex">
<span>def <span class="ident">delete_all</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Deletes everything in pachyderm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_all(self):
    &#34;&#34;&#34;
    Deletes everything in pachyderm.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;DeleteAll&#34;,
        req=pps_proto.google_dot_protobuf_dot_empty__pb2.Empty(),
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.delete_all_pipelines"><code class="name flex">
<span>def <span class="ident">delete_all_pipelines</span></span>(<span>self, force=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Deletes all pipelines.</p>
<p>Params:</p>
<ul>
<li><code>force</code>: Whether to force delete.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_all_pipelines(self, force=None):
    &#34;&#34;&#34;
    Deletes all pipelines.

    Params:

    * `force`: Whether to force delete.
    &#34;&#34;&#34;
    return self._req(Service.PPS, &#34;DeletePipeline&#34;, all=True, force=force)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.delete_job"><code class="name flex">
<span>def <span class="ident">delete_job</span></span>(<span>self, job_id)</span>
</code></dt>
<dd>
<div class="desc"><p>Deletes a job by its ID.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: The ID of the job to delete.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_job(self, job_id):
    &#34;&#34;&#34;
    Deletes a job by its ID.

    Params:

    * `job_id`: The ID of the job to delete.
    &#34;&#34;&#34;
    return self._req(Service.PPS, &#34;DeleteJob&#34;, job=pps_proto.Job(id=job_id))</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.delete_pipeline"><code class="name flex">
<span>def <span class="ident">delete_pipeline</span></span>(<span>self, pipeline_name, force=None, keep_repo=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Deletes a pipeline.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
<li><code>force</code>: Whether to force delete.</li>
<li><code>keep_repo</code>: Whether to keep the repo.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_pipeline(self, pipeline_name, force=None, keep_repo=None):
    &#34;&#34;&#34;
    Deletes a pipeline.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    * `force`: Whether to force delete.
    * `keep_repo`: Whether to keep the repo.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;DeletePipeline&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        force=force,
        keep_repo=keep_repo,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.delete_secret"><code class="name flex">
<span>def <span class="ident">delete_secret</span></span>(<span>self, secret_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Deletes a new secret.</p>
<p>Params:</p>
<ul>
<li><code>secret_name</code>: The name of the secret to delete.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_secret(self, secret_name):
    &#34;&#34;&#34;
    Deletes a new secret.

    Params:

    * `secret_name`: The name of the secret to delete.
    &#34;&#34;&#34;
    secret = pps_proto.Secret(name=secret_name)
    return self._req(Service.PPS, &#34;DeleteSecret&#34;, secret=secret)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.flush_job"><code class="name flex">
<span>def <span class="ident">flush_job</span></span>(<span>self, commits, pipeline_names=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Blocks until all of the jobs which have a set of commits as
provenance have finished. Yields <code>JobInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li><code>commits</code>: A list of tuples, strings, or <code>Commit</code> objects
representing the commits to flush.</li>
<li><code>pipeline_names</code>: An optional list of strings specifying pipeline
names. If specified, only jobs within these pipelines will be flushed.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flush_job(self, commits, pipeline_names=None):
    &#34;&#34;&#34;
    Blocks until all of the jobs which have a set of commits as
    provenance have finished. Yields `JobInfo` objects.

    Params:

    * `commits`: A list of tuples, strings, or `Commit` objects
    representing the commits to flush.
    * `pipeline_names`: An optional list of strings specifying pipeline
    names. If specified, only jobs within these pipelines will be flushed.
    &#34;&#34;&#34;
    if pipeline_names is not None:
        to_pipelines = [pps_proto.Pipeline(name=name) for name in pipeline_names]
    else:
        to_pipelines = None

    return self._req(
        Service.PPS,
        &#34;FlushJob&#34;,
        commits=[commit_from(c) for c in commits],
        to_pipelines=to_pipelines,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.get_job_logs"><code class="name flex">
<span>def <span class="ident">get_job_logs</span></span>(<span>self, job_id, data_filters=None, datum=None, follow=None, tail=None, use_loki_backend=None, since=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets logs for a job. Yields <code>LogMessage</code> objects.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: A string representing a job to get logs of.</li>
<li><code>data_filters</code>: An optional iterable of strings specifying the names
of input files from which we want processing logs. This may contain
multiple files, to query pipelines that contain multiple inputs. Each
filter may be an absolute path of a file within a pps repo, or it may
be a hash for that file (to search for files at specific versions.)</li>
<li><code>datum</code>: An optional <code>Datum</code> object.</li>
<li><code>follow</code>: An optional bool specifying whether logs should continue to
stream forever.</li>
<li><code>tail</code>: An optional int. If nonzero, the number of lines from the end
of the logs to return.
Note: tail applies per container, so you will
get tail * <number of pods> total lines back.</li>
<li><code>use_loki_backend</code>: Whether to use loki as a backend for fetching
logs. Requires a loki-enabled cluster.</li>
<li><code>since</code>: An optional <code>Duration</code> object specifying the start time for
returned logs</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_job_logs(
    self,
    job_id,
    data_filters=None,
    datum=None,
    follow=None,
    tail=None,
    use_loki_backend=None,
    since=None,
):
    &#34;&#34;&#34;
    Gets logs for a job. Yields `LogMessage` objects.

    Params:

    * `job_id`: A string representing a job to get logs of.
    * `data_filters`: An optional iterable of strings specifying the names
      of input files from which we want processing logs. This may contain
      multiple files, to query pipelines that contain multiple inputs. Each
      filter may be an absolute path of a file within a pps repo, or it may
      be a hash for that file (to search for files at specific versions.)
    * `datum`: An optional `Datum` object.
    * `follow`: An optional bool specifying whether logs should continue to
      stream forever.
    * `tail`: An optional int. If nonzero, the number of lines from the end
      of the logs to return.  Note: tail applies per container, so you will
      get tail * &lt;number of pods&gt; total lines back.
    * `use_loki_backend`: Whether to use loki as a backend for fetching
      logs. Requires a loki-enabled cluster.
    * `since`: An optional `Duration` object specifying the start time for
      returned logs
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;GetLogs&#34;,
        job=pps_proto.Job(id=job_id),
        data_filters=data_filters,
        datum=datum,
        follow=follow,
        tail=tail,
        use_loki_backend=use_loki_backend,
        since=since,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.get_pipeline_logs"><code class="name flex">
<span>def <span class="ident">get_pipeline_logs</span></span>(<span>self, pipeline_name, data_filters=None, master=None, datum=None, follow=None, tail=None, use_loki_backend=None, since=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets logs for a pipeline. Yields <code>LogMessage</code> objects.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing a pipeline to get
logs of.</li>
<li><code>data_filters</code>: An optional iterable of strings specifying the names
of input files from which we want processing logs. This may contain
multiple files, to query pipelines that contain multiple inputs. Each
filter may be an absolute path of a file within a pps repo, or it may
be a hash for that file (to search for files at specific versions.)</li>
<li><code>master</code>: An optional bool.</li>
<li><code>datum</code>: An optional <code>Datum</code> object.</li>
<li><code>follow</code>: An optional bool specifying whether logs should continue to
stream forever.</li>
<li><code>tail</code>: An optional int. If nonzero, the number of lines from the end
of the logs to return.
Note: tail applies per container, so you will
get tail * <number of pods> total lines back.</li>
<li><code>use_loki_backend</code>: Whether to use loki as a backend for fetching
logs. Requires a loki-enabled cluster.</li>
<li><code>since</code>: An optional <code>Duration</code> object specifying the start time for
returned logs</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pipeline_logs(
    self,
    pipeline_name,
    data_filters=None,
    master=None,
    datum=None,
    follow=None,
    tail=None,
    use_loki_backend=None,
    since=None,
):
    &#34;&#34;&#34;
    Gets logs for a pipeline. Yields `LogMessage` objects.

    Params:

    * `pipeline_name`: A string representing a pipeline to get
      logs of.
    * `data_filters`: An optional iterable of strings specifying the names
      of input files from which we want processing logs. This may contain
      multiple files, to query pipelines that contain multiple inputs. Each
      filter may be an absolute path of a file within a pps repo, or it may
      be a hash for that file (to search for files at specific versions.)
    * `master`: An optional bool.
    * `datum`: An optional `Datum` object.
    * `follow`: An optional bool specifying whether logs should continue to
      stream forever.
    * `tail`: An optional int. If nonzero, the number of lines from the end
      of the logs to return.  Note: tail applies per container, so you will
      get tail * &lt;number of pods&gt; total lines back.
    * `use_loki_backend`: Whether to use loki as a backend for fetching
      logs. Requires a loki-enabled cluster.
    * `since`: An optional `Duration` object specifying the start time for
      returned logs
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;GetLogs&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        data_filters=data_filters,
        master=master,
        datum=datum,
        follow=follow,
        tail=tail,
        use_loki_backend=use_loki_backend,
        since=since,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.inspect_datum"><code class="name flex">
<span>def <span class="ident">inspect_datum</span></span>(<span>self, job_id, datum_id)</span>
</code></dt>
<dd>
<div class="desc"><p>Inspects a datum. Returns a <code>DatumInfo</code> object.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: The ID of the job.</li>
<li><code>datum_id</code>: The ID of the datum.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inspect_datum(self, job_id, datum_id):
    &#34;&#34;&#34;
    Inspects a datum. Returns a `DatumInfo` object.

    Params:

    * `job_id`: The ID of the job.
    * `datum_id`: The ID of the datum.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;InspectDatum&#34;,
        datum=pps_proto.Datum(id=datum_id, job=pps_proto.Job(id=job_id)),
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.inspect_job"><code class="name flex">
<span>def <span class="ident">inspect_job</span></span>(<span>self, job_id, block_state=None, output_commit=None, full=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Inspects a job with a given ID. Returns a <code>JobInfo</code>.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: The ID of the job to inspect.</li>
<li><code>block_state</code>: If true, block until the job completes.</li>
<li><code>output_commit</code>: An optional tuple, string, or <code>Commit</code> object
representing an output commit to filter on.</li>
<li><code>full</code>: If true, include worker status.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inspect_job(self, job_id, block_state=None, output_commit=None, full=None):
    &#34;&#34;&#34;
    Inspects a job with a given ID. Returns a `JobInfo`.

    Params:

    * `job_id`: The ID of the job to inspect.
    * `block_state`: If true, block until the job completes.
    * `output_commit`: An optional tuple, string, or `Commit` object
    representing an output commit to filter on.
    * `full`: If true, include worker status.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;InspectJob&#34;,
        job=pps_proto.Job(id=job_id),
        block_state=block_state,
        output_commit=commit_from(output_commit)
        if output_commit is not None
        else None,
        full=full,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.inspect_pipeline"><code class="name flex">
<span>def <span class="ident">inspect_pipeline</span></span>(<span>self, pipeline_name, history=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Inspects a pipeline. Returns a <code>PipelineInfo</code> object.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
<li><code>history</code>: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inspect_pipeline(self, pipeline_name, history=None):
    &#34;&#34;&#34;
    Inspects a pipeline. Returns a `PipelineInfo` object.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    * `history`: An optional int that indicates to return jobs from
    historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    &#34;&#34;&#34;
    pipeline = pps_proto.Pipeline(name=pipeline_name)

    if history is None:
        return self._req(Service.PPS, &#34;InspectPipeline&#34;, pipeline=pipeline)
    else:
        # `InspectPipeline` doesn&#39;t support history, but `ListPipeline`
        # with a pipeline filter does, so we use that here
        pipelines = self._req(
            Service.PPS, &#34;ListPipeline&#34;, pipeline=pipeline, history=history
        ).pipeline_info
        assert len(pipelines) &lt;= 1
        return pipelines[0] if len(pipelines) else None</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.inspect_secret"><code class="name flex">
<span>def <span class="ident">inspect_secret</span></span>(<span>self, secret_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Inspects a secret.</p>
<p>Params:</p>
<ul>
<li><code>secret_name</code>: The name of the secret to inspect.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inspect_secret(self, secret_name):
    &#34;&#34;&#34;
    Inspects a secret.

    Params:

    * `secret_name`: The name of the secret to inspect.
    &#34;&#34;&#34;
    secret = pps_proto.Secret(name=secret_name)
    return self._req(Service.PPS, &#34;InspectSecret&#34;, secret=secret)</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.list_datum"><code class="name flex">
<span>def <span class="ident">list_datum</span></span>(<span>self, job_id=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Lists datums. Yields <code>DatumInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: An optional int specifying the ID of a job. Exactly one of
<code>job_id</code> (real) or <code>input</code> (hypothetical) must be set.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_datum(self, job_id=None):
    &#34;&#34;&#34;
    Lists datums. Yields `DatumInfo` objects.

    Params:

    * `job_id`: An optional int specifying the ID of a job. Exactly one of
      `job_id` (real) or `input` (hypothetical) must be set.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;ListDatum&#34;,
        job=pps_proto.Job(id=job_id),
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.list_job"><code class="name flex">
<span>def <span class="ident">list_job</span></span>(<span>self, pipeline_name=None, input_commit=None, output_commit=None, history=None, full=None, jqFilter=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Lists jobs. Yields <code>JobInfo</code> objects.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: An optional string representing a pipeline name to
filter on.</li>
<li><code>input_commit</code>: An optional list of tuples, strings, or <code>Commit</code>
objects representing input commits to filter on.</li>
<li><code>output_commit</code>: An optional tuple, string, or <code>Commit</code> object
representing an output commit to filter on.</li>
<li><code>history</code>: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
<li><code>full</code>: An optional bool indicating whether the result should
include all pipeline details in each <code>JobInfo</code>, or limited information
including name and status, but excluding information in the pipeline
spec. Leaving this <code>None</code> (or <code>False</code>) can make the call significantly
faster in clusters with a large number of pipelines and jobs. Note
that if <code>input_commit</code> is set, this field is coerced to <code>True</code>.</li>
<li><code>jqFilter</code>: An optional string containing a <code>jq</code> filter that can
restrict the list of jobs returned, for convenience</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_job(
    self,
    pipeline_name=None,
    input_commit=None,
    output_commit=None,
    history=None,
    full=None,
    jqFilter=None,
):
    &#34;&#34;&#34;
    Lists jobs. Yields `JobInfo` objects.

    Params:

    * `pipeline_name`: An optional string representing a pipeline name to
      filter on.
    * `input_commit`: An optional list of tuples, strings, or `Commit`
      objects representing input commits to filter on.
    * `output_commit`: An optional tuple, string, or `Commit` object
      representing an output commit to filter on.
    * `history`: An optional int that indicates to return jobs from
      historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    * `full`: An optional bool indicating whether the result should
      include all pipeline details in each `JobInfo`, or limited information
      including name and status, but excluding information in the pipeline
      spec. Leaving this `None` (or `False`) can make the call significantly
      faster in clusters with a large number of pipelines and jobs. Note
      that if `input_commit` is set, this field is coerced to `True`.
    * `jqFilter`: An optional string containing a `jq` filter that can
      restrict the list of jobs returned, for convenience
    &#34;&#34;&#34;
    if isinstance(input_commit, list):
        input_commit = [commit_from(ic) for ic in input_commit]
    elif input_commit is not None:
        input_commit = [commit_from(input_commit)]

    return self._req(
        Service.PPS,
        &#34;ListJob&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name)
        if pipeline_name is not None
        else None,
        input_commit=input_commit,
        output_commit=commit_from(output_commit)
        if output_commit is not None
        else None,
        history=history,
        full=full,
        jqFilter=jqFilter,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.list_pipeline"><code class="name flex">
<span>def <span class="ident">list_pipeline</span></span>(<span>self, history=None, allow_incomplete=None, jqFilter=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Lists pipelines. Returns a <code>PipelineInfos</code> object.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
<li><code>history</code>: An optional int that indicates to return jobs from
historical versions of pipelines. Semantics are:<ul>
<li>0: Return jobs from the current version of the pipeline or
pipelines.</li>
<li>1: Return the above and jobs from the next most recent version</li>
<li>2: etc.</li>
<li>-1: Return jobs from all historical versions.</li>
</ul>
</li>
<li><code>allow_incomplete</code>: An optional boolean that, if set to <code>True</code>, causes
<code>list_pipeline</code> to return PipelineInfos with incomplete data where the
pipeline spec cannot beretrieved. Incomplete PipelineInfos will have a
nil Transform field, but will have the fields present in
EtcdPipelineInfo.</li>
<li><code>jqFilter</code>: An optional string containing a <code>jq</code> filter that can
restrict the list of jobs returned, for convenience</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_pipeline(self, history=None, allow_incomplete=None, jqFilter=None):
    &#34;&#34;&#34;
    Lists pipelines. Returns a `PipelineInfos` object.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    * `history`: An optional int that indicates to return jobs from
      historical versions of pipelines. Semantics are:
        * 0: Return jobs from the current version of the pipeline or
          pipelines.
        * 1: Return the above and jobs from the next most recent version
        * 2: etc.
        * -1: Return jobs from all historical versions.
    * `allow_incomplete`: An optional boolean that, if set to `True`, causes
      `list_pipeline` to return PipelineInfos with incomplete data where the
      pipeline spec cannot beretrieved. Incomplete PipelineInfos will have a
      nil Transform field, but will have the fields present in
      EtcdPipelineInfo.
    * `jqFilter`: An optional string containing a `jq` filter that can
      restrict the list of jobs returned, for convenience
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;ListPipeline&#34;,
        history=history,
        allow_incomplete=allow_incomplete,
        jqFilter=jqFilter,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.list_secret"><code class="name flex">
<span>def <span class="ident">list_secret</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Lists secrets. Returns a list of <code>SecretInfo</code> objects.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_secret(self):
    &#34;&#34;&#34;
    Lists secrets. Returns a list of `SecretInfo` objects.
    &#34;&#34;&#34;

    return self._req(
        Service.PPS,
        &#34;ListSecret&#34;,
        req=pps_proto.google_dot_protobuf_dot_empty__pb2.Empty(),
    ).secret_info</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.restart_datum"><code class="name flex">
<span>def <span class="ident">restart_datum</span></span>(<span>self, job_id, data_filters=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Restarts a datum.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: The ID of the job.</li>
<li><code>data_filters</code>: An optional iterable of strings.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def restart_datum(self, job_id, data_filters=None):
    &#34;&#34;&#34;
    Restarts a datum.

    Params:

    * `job_id`: The ID of the job.
    * `data_filters`: An optional iterable of strings.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;RestartDatum&#34;,
        job=pps_proto.Job(id=job_id),
        data_filters=data_filters,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.run_cron"><code class="name flex">
<span>def <span class="ident">run_cron</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Explicitly triggers a pipeline with one or more cron inputs to run
now.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_cron(self, pipeline_name):
    &#34;&#34;&#34;
    Explicitly triggers a pipeline with one or more cron inputs to run
    now.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    &#34;&#34;&#34;

    return self._req(
        Service.PPS,
        &#34;RunCron&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.run_pipeline"><code class="name flex">
<span>def <span class="ident">run_pipeline</span></span>(<span>self, pipeline_name, provenance=None, job_id=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs a pipeline.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
<li><code>provenance</code>: An optional iterable of <code>CommitProvenance</code> objects
representing the pipeline execution provenance.</li>
<li><code>job_id</code>: An optional string specifying a specific job ID to run.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_pipeline(self, pipeline_name, provenance=None, job_id=None):
    &#34;&#34;&#34;
    Runs a pipeline.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    * `provenance`: An optional iterable of `CommitProvenance` objects
    representing the pipeline execution provenance.
    * `job_id`: An optional string specifying a specific job ID to run.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;RunPipeline&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
        provenance=provenance,
        job_id=job_id,
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.start_pipeline"><code class="name flex">
<span>def <span class="ident">start_pipeline</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Starts a pipeline.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_pipeline(self, pipeline_name):
    &#34;&#34;&#34;
    Starts a pipeline.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;StartPipeline&#34;,
        pipeline=pps_proto.Pipeline(name=pipeline_name),
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.stop_job"><code class="name flex">
<span>def <span class="ident">stop_job</span></span>(<span>self, job_id, output_commit=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Stops a job by its ID.</p>
<p>Params:</p>
<ul>
<li><code>job_id</code>: The ID of the job to stop.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_job(self, job_id, output_commit=None):
    &#34;&#34;&#34;
    Stops a job by its ID.

    Params:

    * `job_id`: The ID of the job to stop.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS,
        &#34;StopJob&#34;,
        job=pps_proto.Job(id=job_id),
        output_commit=commit_from(output_commit),
    )</code></pre>
</details>
</dd>
<dt id="python_pachyderm.mixin.pps.PPSMixin.stop_pipeline"><code class="name flex">
<span>def <span class="ident">stop_pipeline</span></span>(<span>self, pipeline_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Stops a pipeline.</p>
<p>Params:</p>
<ul>
<li><code>pipeline_name</code>: A string representing the pipeline name.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_pipeline(self, pipeline_name):
    &#34;&#34;&#34;
    Stops a pipeline.

    Params:

    * `pipeline_name`: A string representing the pipeline name.
    &#34;&#34;&#34;
    return self._req(
        Service.PPS, &#34;StopPipeline&#34;, pipeline=pps_proto.Pipeline(name=pipeline_name)
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="python_pachyderm.mixin" href="index.html">python_pachyderm.mixin</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="python_pachyderm.mixin.pps.pipeline_inputs" href="#python_pachyderm.mixin.pps.pipeline_inputs">pipeline_inputs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="python_pachyderm.mixin.pps.PPSMixin" href="#python_pachyderm.mixin.pps.PPSMixin">PPSMixin</a></code></h4>
<ul class="">
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.create_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.create_pipeline">create_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.create_pipeline_from_request" href="#python_pachyderm.mixin.pps.PPSMixin.create_pipeline_from_request">create_pipeline_from_request</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.create_secret" href="#python_pachyderm.mixin.pps.PPSMixin.create_secret">create_secret</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.create_tf_job_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.create_tf_job_pipeline">create_tf_job_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.delete_all" href="#python_pachyderm.mixin.pps.PPSMixin.delete_all">delete_all</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.delete_all_pipelines" href="#python_pachyderm.mixin.pps.PPSMixin.delete_all_pipelines">delete_all_pipelines</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.delete_job" href="#python_pachyderm.mixin.pps.PPSMixin.delete_job">delete_job</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.delete_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.delete_pipeline">delete_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.delete_secret" href="#python_pachyderm.mixin.pps.PPSMixin.delete_secret">delete_secret</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.flush_job" href="#python_pachyderm.mixin.pps.PPSMixin.flush_job">flush_job</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.get_job_logs" href="#python_pachyderm.mixin.pps.PPSMixin.get_job_logs">get_job_logs</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.get_pipeline_logs" href="#python_pachyderm.mixin.pps.PPSMixin.get_pipeline_logs">get_pipeline_logs</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.inspect_datum" href="#python_pachyderm.mixin.pps.PPSMixin.inspect_datum">inspect_datum</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.inspect_job" href="#python_pachyderm.mixin.pps.PPSMixin.inspect_job">inspect_job</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.inspect_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.inspect_pipeline">inspect_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.inspect_secret" href="#python_pachyderm.mixin.pps.PPSMixin.inspect_secret">inspect_secret</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.list_datum" href="#python_pachyderm.mixin.pps.PPSMixin.list_datum">list_datum</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.list_job" href="#python_pachyderm.mixin.pps.PPSMixin.list_job">list_job</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.list_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.list_pipeline">list_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.list_secret" href="#python_pachyderm.mixin.pps.PPSMixin.list_secret">list_secret</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.restart_datum" href="#python_pachyderm.mixin.pps.PPSMixin.restart_datum">restart_datum</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.run_cron" href="#python_pachyderm.mixin.pps.PPSMixin.run_cron">run_cron</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.run_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.run_pipeline">run_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.start_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.start_pipeline">start_pipeline</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.stop_job" href="#python_pachyderm.mixin.pps.PPSMixin.stop_job">stop_job</a></code></li>
<li><code><a title="python_pachyderm.mixin.pps.PPSMixin.stop_pipeline" href="#python_pachyderm.mixin.pps.PPSMixin.stop_pipeline">stop_pipeline</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>